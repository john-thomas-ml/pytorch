{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fd71d41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0816517e",
   "metadata": {},
   "source": [
    "f(x) = wx + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37659dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1,2,3,4,5,6,7,8],dtype = torch.float32)\n",
    "y = torch.tensor([3,5,7,9,11,13,15,17],dtype = torch.float32) #y = 2x + 1\n",
    "w = torch.tensor(0.0,dtype = torch.float32,requires_grad = True)\n",
    "b = torch.tensor(0.0,dtype = torch.float32, requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d5317bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x):\n",
    "    return w * x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8156aeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y,y_pred):\n",
    "    return ((y_pred - y) ** 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "109c589b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = 5.0 \n",
    "forward(X_test).item() #forward basically returns prediction , item converts from tensor to python float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5856943e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000: w = 2.000002145767212, b = 0.9999890327453613 loss = 2.4421353828074643e-11\n",
      "Epoch 2000: w = 2.000002145767212, b = 0.9999890327453613 loss = 2.4421353828074643e-11\n",
      "Epoch 3000: w = 2.000002145767212, b = 0.9999890327453613 loss = 2.4421353828074643e-11\n",
      "Epoch 4000: w = 2.000002145767212, b = 0.9999890327453613 loss = 2.4421353828074643e-11\n",
      "Epoch 5000: w = 2.000002145767212, b = 0.9999890327453613 loss = 2.4421353828074643e-11\n",
      "Epoch 6000: w = 2.000002145767212, b = 0.9999890327453613 loss = 2.4421353828074643e-11\n",
      "Epoch 7000: w = 2.000002145767212, b = 0.9999890327453613 loss = 2.4421353828074643e-11\n",
      "Epoch 8000: w = 2.000002145767212, b = 0.9999890327453613 loss = 2.4421353828074643e-11\n",
      "Epoch 9000: w = 2.000002145767212, b = 0.9999890327453613 loss = 2.4421353828074643e-11\n",
      "Epoch 10000: w = 2.000002145767212, b = 0.9999890327453613 loss = 2.4421353828074643e-11\n",
      "Prediction after training 5.0 = 11.0\n",
      "Weight is 2.000002145767212, bias is 0.9999890327453613\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "epochs = 10000\n",
    "\n",
    "for i in range(epochs):\n",
    "    y_pred = forward(x)\n",
    "\n",
    "    l = loss(y,y_pred) #calculates loss\n",
    "\n",
    "    l.backward() # does dl/dw  and dl/db and is stored in w.grad and b.grad\n",
    "\n",
    "    with torch.no_grad(): #doesnt use this for computing gradients\n",
    "        w -= learning_rate * w.grad\n",
    "        b -= learning_rate * b.grad\n",
    "\n",
    "    w.grad.zero_() #zero the gradients\n",
    "    b.grad.zero_()\n",
    "\n",
    "    if (i + 1) % 1000 == 0:\n",
    "        print(f\"Epoch {i+1}: w = {w.item()}, b = {b.item()} loss = {l.item()}\")\n",
    "\n",
    "print(f\"Prediction after training {X_test} = {forward(X_test).item()}\") #final output\n",
    "print(f\"Weight is {w}, bias is {b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3e0e1d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d10f2884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor( [ [1],[2],[3],[4],[5],[6],[7],[8] ], dtype = torch.float32) #nn expects 2d tensors\n",
    "y = torch.tensor( [ [3],[5],[7],[9],[11],[13],[15],[17] ], dtype = torch.float32)\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2f68c91b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape[0] #no of samples\n",
    "x.shape[1] #input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e1e25a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = torch.tensor( [ [5] ],dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09fa48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training prediction 5.0 = 3.670987129211426\n"
     ]
    }
   ],
   "source": [
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self,input_dim,output_dim):\n",
    "        super().__init__() #tracks paramenters and stuff and basically setting up \n",
    "        self.lin = nn.Linear(input_dim,output_dim) #creates the weight and bias\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.lin(x) #performs wx + b\n",
    "\n",
    "model = LinearRegression(x.shape[1],x.shape[1]) #Expects the i/p no of features and o/p no of features\n",
    "\n",
    "print(f\"Before training prediction {x_test.item()} = {model(x_test).item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "5499478d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[0.7669]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.1636], requires_grad=True)]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "58ef00bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1000: w = 2.002980947494507 , b = 0.9832407832145691, loss = 5.830454392707907e-05 \n",
      "Epoch: 2000: w = 2.0000545978546143 , b = 0.999692440032959, loss = 1.9630910230716836e-08 \n",
      "Epoch: 3000: w = 2.000002145767212 , b = 0.9999890327453613, loss = 2.4421353828074643e-11 \n",
      "Epoch: 4000: w = 2.000002145767212 , b = 0.9999890327453613, loss = 2.4421353828074643e-11 \n",
      "Epoch: 5000: w = 2.000002145767212 , b = 0.9999890327453613, loss = 2.4421353828074643e-11 \n",
      "Epoch: 6000: w = 2.000002145767212 , b = 0.9999890327453613, loss = 2.4421353828074643e-11 \n",
      "Epoch: 7000: w = 2.000002145767212 , b = 0.9999890327453613, loss = 2.4421353828074643e-11 \n",
      "Epoch: 8000: w = 2.000002145767212 , b = 0.9999890327453613, loss = 2.4421353828074643e-11 \n",
      "Epoch: 9000: w = 2.000002145767212 , b = 0.9999890327453613, loss = 2.4421353828074643e-11 \n",
      "Epoch: 10000: w = 2.000002145767212 , b = 0.9999890327453613, loss = 2.4421353828074643e-11 \n",
      "Prediction after training 5.0, = 11.0\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "epoch = 10000\n",
    "\n",
    "\n",
    "loss = nn.MSELoss() #mean square error loss\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr = learning_rate) #Stochastic gradient descent\n",
    "#model parameters include the weights and biases\n",
    "\n",
    "for i in range(epoch):\n",
    "    y_pred = model(x) #internally calls the forward fucnction\n",
    "\n",
    "    l = loss(y,y_pred) #calls the mse loss thingy to calculate loss\n",
    "\n",
    "    l.backward() #calculate gradient\n",
    "\n",
    "    optimizer.step() #changes weights and biases\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if (i + 1) % 1000 == 0:\n",
    "        w,b = model.parameters() #parameters unpacking\n",
    "        print(f\"Epoch: {i+1}: w = {w.item()} , b = {b.item()}, loss = {l.item()} \")\n",
    "\n",
    "print(f\"Prediction after training {x_test.item()}, = {model(x_test).item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e586b29e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[2.0000]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1.0000], requires_grad=True)]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters()) #2x + 1 perfect"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "(venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
